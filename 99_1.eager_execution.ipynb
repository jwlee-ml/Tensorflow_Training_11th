{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forked from https://github.com/zaidalyafeai/Notebooks/blob/master/Eager_Execution_Enabled.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M7r1fJQniHj7"
   },
   "source": [
    "# Introduction \n",
    "\n",
    "Eager Execution (EE) enables you to run operations immediately. As we know in TensorFlow, you have to create a graph and run it within a session in order to execute the operations of the graph. On the other hand, EE enables you to run operations directly and inspect the output as the operations are executed. This is very useful especially for debugging. Moreover, EE is pythonic and intergrates pretty well with numpy so it makes programming really easy and flexible. The next version of TenosrFlow \"2.0\" will enable EE by default. \n",
    "\n",
    "From Google AI [article](https://ai.googleblog.com/2017/10/eager-execution-imperative-define-by.html), these are some benefits of using EE\n",
    "\n",
    "   * Fast debugging with immediate run-time errors and integration with  python tools\n",
    "   * Support for dynamic models using easy-to-use Python control flow\n",
    "   * Strong support for custom and higher-order gradients\n",
    "   * Almost all of the available TensorFlow operations\n",
    "   \n",
    "   ![alt text](https://i.imgur.com/YUlhihi.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CiwZoGf85dnU"
   },
   "source": [
    "# Enabling Eager Execution \n",
    "In current versions of TensorFlow eager execution is not enabled by default so you have to enable it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jlnQG8hC-uCg"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8Fq5Gb135Z5f"
   },
   "source": [
    "Check if eager execution is enabled "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "kS7er1hy-7yO",
    "outputId": "5227d41b-3f1a-49f1-92a1-9405df8dbe04"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UC6X5Y844_E-"
   },
   "source": [
    "# Executing Ops Eagerly \n",
    "By perfoming operations you can see the output directly without creating a session. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "DmwZJKlA_B15",
    "outputId": "d1ec0347-5054-4acf-d110-f8dcc0d94802"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[4.]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = [[2.]]\n",
    "m = tf.square(x)\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WGLoIPrQ6ZYT"
   },
   "source": [
    "You can call `.numpy` to retrieve the results of the tensor as a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-FGFGbZq6fRo",
    "outputId": "d7378a89-55f6-42bb-ee2e-3b8db98e6fe2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l9Xlu7in6m22"
   },
   "source": [
    "You can also compute an operation including two tensors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "d4tKJJ90_QMM",
    "outputId": "92fb99d1-c57d-4ac4-8f18-1d405d9b60df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a * b = \n",
      " [[ 8  9]\n",
      " [18 19]]\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([[1, 2],\n",
    "                 [3, 4]])\n",
    "\n",
    "b = tf.constant([[2, 1],\n",
    "                 [3, 4]])\n",
    "\n",
    "ab = tf.matmul(a, b)\n",
    "\n",
    "print('a * b = \\n', ab.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V5qlVJygETcb"
   },
   "source": [
    "# Constants and Variables \n",
    "\n",
    "\n",
    "*   `tf.constant`, creates a constant tensor populated with the values as argument. The values are immutable. \n",
    "*   `tf.Variable `, this method encapsultes a mutable tensor that can be changed later using assign \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ayMVXFj1FZxz"
   },
   "source": [
    "Creating a constant tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "g2KFQKSLFNlS",
    "outputId": "0b74dac1-9c3d-4a03-9f91-9f63dddb3886"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[2 3]], shape=(1, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([[2,3]])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mn8uX4t5FtHp"
   },
   "source": [
    "A constant tensor is immutable so you cannot assign a new value to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xrJqeZfgHU-j",
    "outputId": "ace27460-d9bd-480d-be05-f47962d1ae1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception raised \n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    a.assign([[3,4]])\n",
    "except:\n",
    "    print('Exception raised ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lrgbhCu8H3rm"
   },
   "source": [
    "On the other hand variables are mutable and can be assigned a new value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "WSMIotOQFw2F",
    "outputId": "adf3a704-f683-4d48-a0d5-7b3728f15d2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old value for v = 5.0\n",
      "New value for v = 2.0\n"
     ]
    }
   ],
   "source": [
    "v = tf.Variable(5.)\n",
    "\n",
    "print('Old value for v =', v.numpy())\n",
    "v.assign(2.)\n",
    "print('New value for v =', v.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "se0MFrEwMXWo"
   },
   "source": [
    "You can also increment/decrement the value of a tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "d9M50PpdMzEn",
    "outputId": "1c135c36-0a0d-4a0f-d885-6b8d7e53ffd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value     :  2.0\n",
      "increment :  3.0\n",
      "decrement :  2.0\n"
     ]
    }
   ],
   "source": [
    "v.assign(2.)\n",
    "print('value     : ', v.numpy())\n",
    "print('increment : ', tf.assign_add(v, 1).numpy())\n",
    "print('decrement : ', tf.assign_sub(v, 1).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FtGAYUUWI8bX"
   },
   "source": [
    "You can return many information from a tensor variable, like the name, type , shape and what device it executes on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "2wQMtx3QJBSg",
    "outputId": "68daaa75-d2ca-4919-b250-3133f6eb22c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name  :  Variable:0\n",
      "type  :  <dtype: 'float32'>\n",
      "shape :  ()\n",
      "device:  /job:localhost/replica:0/task:0/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "print('name  : ', v.name)\n",
    "print('type  : ', v.dtype)\n",
    "print('shape : ', v.shape)\n",
    "print('device: ', v.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5irSm-yDN0nV"
   },
   "source": [
    "# Gradient Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y36ig_TVAAoM"
   },
   "source": [
    "Gradient evaluation is very importnat machine learning because it is based on function optimization. You can use `tf.GradientTape()` method to record the gradient of an arbitrary function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "bdZmXyAi_3M3",
    "outputId": "e829b23f-62bd-4e8b-8e0f-654c2608766c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gradient of w^2 at 2.0 is 4.0\n"
     ]
    }
   ],
   "source": [
    "w = tf.Variable(2.0)\n",
    "\n",
    "#watch the gradient of the loss operation\n",
    "with tf.GradientTape() as tape:\n",
    "    loss = w * w\n",
    "\n",
    "grad = tape.gradient(loss, w)\n",
    "print(f'The gradient of w^2 at {w.numpy()} is {grad.numpy()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qEMEMfV8Pwvt"
   },
   "source": [
    "You also compute the gradient directly using `gradients_function`. In this example we evaluate the gradient of the sigmoid function \n",
    "\n",
    "$$f(x) = \\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "Note that \n",
    "\n",
    "$$f'(x) = \\frac{e^{-x}}{(1+e^{-x})^2} = f(x)(1-f(x)) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "nrw-DuoWP0A6",
    "outputId": "85df0b9c-d977-48e2-a866-26b86ddb8ba9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gradient of the sigmoid function at 2.0 is  0.104993574\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.contrib.eager as tfe \n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + tf.exp(-x))\n",
    "\n",
    "grad_sigmoid = tfe.gradients_function(sigmoid)\n",
    "\n",
    "print('The gradient of the sigmoid function at 2.0 is ', grad_sigmoid(2.0)[0].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jew_BsZaYeVz"
   },
   "source": [
    "You can also compute higher order derivatives by nesting a gradient functions. For instance, \n",
    "\n",
    "$$f(x) = \\log(x) , f'(x) = \\frac{1}{x}, f''(x) = \\frac{-1}{x^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "UoFFIr_AXUnm",
    "outputId": "ea684bb1-e807-4a18-e1ff-ceae70d3884c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first  derivative of log at x = 1 is  1.0\n",
      "The second derivative of log at x = 1 is  -1.0\n",
      "The third  derivative of log at x = 1 is  2.0\n"
     ]
    }
   ],
   "source": [
    "dx = tfe.gradients_function\n",
    "\n",
    "def log(x):\n",
    "    return tf.log(x)\n",
    "\n",
    "dx_log = dx(log)\n",
    "dx2_log = dx(dx(log))\n",
    "dx3_log = dx(dx(dx(log)))\n",
    "\n",
    "print('The first  derivative of log at x = 1 is ', dx_log(1.)[0].numpy())\n",
    "print('The second derivative of log at x = 1 is ', dx2_log(1.)[0].numpy())\n",
    "print('The third  derivative of log at x = 1 is ', dx3_log(1.)[0].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hbIHKtEEbujP"
   },
   "source": [
    "# Linear Regression \n",
    "\n",
    "This example is refactored from https://www.tensorflow.org/guide/eager. We create a complete example of using linear regression to predict the paramters of the function \n",
    "\n",
    "$$f(x) = 3 x + 2 + noise$$\n",
    "\n",
    "Given a point $x$ we want to predict the value of $y$. We train the model on 1000 data pairs $(x,f(x))$. \n",
    "\n",
    "The model to learn is a linear model \n",
    "\n",
    "$$\\hat{y} = W x + b$$\n",
    "\n",
    "Note that, we use `tf.GradientTape` to record the gradient with respect our trainable paramters.  \n",
    "\n",
    "We MSE to calcuate the loss \n",
    "\n",
    "$$g = (y-\\hat{y})^2$$\n",
    "\n",
    "We use Gradient Descent to update the paramters \n",
    "\n",
    "$$W = W - \\alpha  \\frac{\\partial g}{\\partial W}$$\n",
    "\n",
    "$$b = b - \\alpha  \\frac{\\partial g}{\\partial b}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "colab_type": "code",
    "id": "P7A5lcylAERT",
    "outputId": "bc72f562-bfb3-470c-855a-0754cb6ee968"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 000: 14.478\n",
      "Loss at step 020: 6.947\n",
      "Loss at step 040: 3.632\n",
      "Loss at step 060: 2.173\n",
      "Loss at step 080: 1.531\n",
      "Loss at step 100: 1.248\n",
      "Loss at step 120: 1.124\n",
      "Loss at step 140: 1.069\n",
      "Loss at step 160: 1.045\n",
      "Loss at step 180: 1.035\n",
      "W : 2.9950246810913086 , b  = 1.9608360528945923 \n"
     ]
    }
   ],
   "source": [
    "#1000 data points \n",
    "NUM_EXAMPLES = 1000\n",
    "\n",
    "#define inputs and outputs with some noise \n",
    "X = tf.random_normal([NUM_EXAMPLES])  #inputs \n",
    "noise = tf.random_normal([NUM_EXAMPLES]) #noise \n",
    "y = X * 3 + 2 + noise  #true output\n",
    "\n",
    "#create model paramters with initial values \n",
    "W = tf.Variable(0.)\n",
    "b = tf.Variable(0.)\n",
    "\n",
    "#training info\n",
    "train_steps = 200\n",
    "learning_rate = 0.01\n",
    "\n",
    "for i in range(train_steps):\n",
    "  \n",
    "  #watch the gradient flow \n",
    "    with tf.GradientTape() as tape:\n",
    "    \n",
    "    #forward pass \n",
    "    yhat = X * W + b\n",
    "    \n",
    "    #calcuate the loss (difference squared error)\n",
    "    error = yhat - y\n",
    "    loss = tf.reduce_mean(tf.square(error))\n",
    "  \n",
    "    #evalute the gradient with the respect to the paramters\n",
    "    dW, db = tape.gradient(loss, [W, b])\n",
    "\n",
    "    #update the paramters using Gradient Descent  \n",
    "    W.assign_sub(dW * learning_rate)\n",
    "    b.assign_sub(db* learning_rate)\n",
    "\n",
    "    #print the loss every 20 iterations \n",
    "    if i % 20 == 0:\n",
    "    print(\"Loss at step {:03d}: {:.3f}\".format(i, loss))\n",
    "      \n",
    "print(f'W : {W.numpy()} , b  = {b.numpy()} ')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Eager Execution Enabled.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
